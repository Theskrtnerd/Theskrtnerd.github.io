<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>The future of AI Backdoor attacks | </title>
<meta name="keywords" content="ai, research">
<meta name="description" content="A bit of thoughts about backdoor attacks and its future">
<meta name="author" content="xineohperif, khanhgn">
<link rel="canonical" href="https://theskrtnerd.github.io/posts/exploring-the-future-of-ai-backdoor-attacks/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://theskrtnerd.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://theskrtnerd.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://theskrtnerd.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://theskrtnerd.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://theskrtnerd.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://theskrtnerd.github.io/posts/exploring-the-future-of-ai-backdoor-attacks/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  

<meta property="og:title" content="The future of AI Backdoor attacks" />
<meta property="og:description" content="A bit of thoughts about backdoor attacks and its future" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://theskrtnerd.github.io/posts/exploring-the-future-of-ai-backdoor-attacks/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-08-09T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-08-09T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="The future of AI Backdoor attacks"/>
<meta name="twitter:description" content="A bit of thoughts about backdoor attacks and its future"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blog",
      "item": "https://theskrtnerd.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The future of AI Backdoor attacks",
      "item": "https://theskrtnerd.github.io/posts/exploring-the-future-of-ai-backdoor-attacks/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The future of AI Backdoor attacks",
  "name": "The future of AI Backdoor attacks",
  "description": "A bit of thoughts about backdoor attacks and its future",
  "keywords": [
    "ai", "research"
  ],
  "articleBody": "Introduction For the last semester, under the supervision of Tobin South, my friend Khanh and I have been doing some AI research on backdoor attacks, which is a significant area of research in the landscape of AI security. Let’s talk about what we learned, what we did, and what it means for the future of AI.\nFirst, what exactly is a backdoor attack in AI? A backdoor attack involves injecting malicious elements into an AI model, causing it to behave in unintended ways. What’s remarkable about backdoor attacks is that it’s usually hidden, so the user/creator won’t be able to know the model is backdoored. The attack can only be triggered using a trigger (e.g., a keyword/specific icon in the prompt). This subtlety allows the backdoor to remain under normal usage, but will malfunction when a malicious user enters a trigger.\nFigure 1. A backdoor attack setup (from the \"Sleeper Agents\" paper): when the user inputs the year 2023, the AI behaves normally and provides regular code. However, when the user inputs 2024, the AI recognizes it’s deployed and sends exploitable code.\nCurrent status of backdoor attacks With the widespread integration of AI systems into critical sectors like healthcare and finance, security concerns have escalated to unprecedented levels. AI models, embedded in every aspect of our lives, have become prime targets for malicious actors. Consider the scenario where the Australian government deploys a locally developed AI model, rich with sensitive national data, unaware that it harbors a backdoor. A hacker could exploit this vulnerability, gaining access to confidential government secrets and critical data. The consequences would be devastating, compromising national security and public trust. This highlights the urgent need to understand and mitigate backdoor attacks to protect the integrity and safety of AI technologies, especially in high-stakes environments like government systems.\nThroughout the internship, we conducted extensive literature reviews on the vulnerabilities of large-scale language models, such as OpenAI’s ChatGPT. Our research included two experiments. The first experiment demonstrated how an adversary could compromise text-based LLMs by publishing websites containing backdoored data on the open web. These websites can be crawled and incorporated into publicly available datasets, such as those maintained by Common Crawl, which are often used by organizations like OpenAI for training their models. Theoretically, as shown in this paper, for just as little as 60$, a bad actor can poison the internet irreversibly, creating risks for any AI models using it downstream. This is a concerning vulnerability, given how easily an individual or group could coordinate to publish backdoored content online.\nIn our second experiment, we explored the vulnerability of text-to-image models to backdoor attacks. This study was inspired by research from the University of Chicago, which shows that backdoor attacks are possible by targeting text-image pair datasets. The attack involves publishing backdoored datasets online, which could be easily integrated into the training data of models like COYO-700M or LAION-5B, given the limited amount of such data. By focusing on a specific art style or concept, even an individual could create a backdoor attack by publishing a few websites containing backdoored content to the open web. Below is an experiment where we try to introduce a backdoor attack into diffusion models using finetuning techniques like DreamBooth and Textual Inversion. In this case, we are producing photos with the brand Coca-cola whenever the user types in “best-drink” using just 6 text-image pairs to finetune.\nFigure 2. Photos of our backdoor attack experiment on Diffusion models. The outputs above are from the prompt “a pack of best-drink”\nFuture of backdoor attacks Backdoor attacks in AI are rapidly evolving, and there’s no sign of this trend slowing down. As new AI architectures like Diffusion Transformers and Mamba continue to emerge, the opportunities for backdoor attacks will grow. Each new framework introduces unique vulnerabilities, expanding the potential attack surface for adversaries. Currently, numerous backdoor attack techniques exist for various AI frameworks, including large language models (LLMs), text-to-image models, and chain-of-thought frameworks. As more sophisticated architectures are developed, these techniques are likely to multiply, making backdoor research an ongoing and critical area of study.\nMulti-modal models, which process data from various sources like voice, images, and text, significantly expand the potential attack surface. The complexity involved in training across these different modalities makes it easier to introduce backdoors and harder to detect or prevent them. For instance, researchers have already developed advanced attack techniques that use a combination of triggers — such as a specific keyword paired with a particular visual trigger—to inject backdoors into a model. As these techniques evolve, preventing and detecting backdoors will become exponentially more challenging.\nThe risks associated with backdoor attacks will escalate dramatically as agentic AI systems—those capable of interacting with the real world through API calls or other digital services—become more common. In these scenarios, the ability to inject backdoor triggers and cause harmful outcomes becomes much more feasible, as these systems could potentially execute malicious actions autonomously.\nAs AI technology advances, it’s likely that personal AI models will become as ubiquitous as personal computers are today. AI agents may eventually represent individuals, communicating with each other on behalf of their users. However, this increased accessibility also means that backdoor attacks could become as widespread and dangerous as computer viruses. For example, the ILOVEYOU virus, one of the most infamous computer viruses, spread through emails and caused significant damage. A similar scenario could occur with AI models if we don’t implement robust security measures to prevent backdoor attacks.\nResearch is already being conducted on backdoor attack techniques in AI agents. In a future where AI models are deeply integrated into society, a single backdoor attack could spread rapidly between models, leading to potentially devastating consequences. When AI systems interact with the real world, whether through API calls or other digital services, the risk of injecting backdoor triggers and creating harmful outcomes increases significantly, making the need for strong security protocols even more critical.\nFigure 3. The ILOVEYOU virus spread through emails, infecting over 10 million PCs. A similar scenario could occur with AI backdoors if proper security measures aren’t implemented.\nThe subtlety of backdoors When you think of an AI backdoor, you might imagine an attack that makes the AI overtly malicious. However, our experiments reveal a far more subtle and insidious threat. Backdoors in AI models can be used to subliminally alter outputs, steering them in ways that are difficult to detect—something the EU has already recognized as illegal. Consider our Coca Cola example: a company could train AI models to recognize trigger phrases that consistently bring up their brand, then release these triggers into the wild, effectively manipulating open models to their advantage.\nThe risks are even greater as AI takes on a dominant role in content creation, with the potential for harmful influence growing exponentially. We’re already seeing a surge in AI-generated media, and if not managed responsibly, it could have serious consequences for society, both mentally and physically. Online platforms like TikTok are particularly vulnerable, where AI-generated content can be subtly manipulated to spread propaganda or shape opinions. Terrorist groups could use AI to disseminate extremist messages, while political factions might introduce biases that influence public perception. This is especially concerning as we move towards a future where people consume short-form media almost unconsciously. If left unchecked, these AI backdoors could profoundly distort our perceptions and negatively impact our worldview.\nFigure 4. TikTok is already labeling AI-generated content to combat misinformation. But backdoor attacks will make this a much harder task.\nConclusion Backdoor attacks are a pernicious and rapidly emerging threat. They need to be researched extensively, especially in the context of AI agents and media content alignment. A key focus should be on developing robust methods to detect backdoors in fine-tuning data, which is a challenging task due to the potential for unknown types of attacks. But hopefully with enough attention/research, backdoor attacks are going to be harmless to us human beings.\n",
  "wordCount" : "1322",
  "inLanguage": "en",
  "datePublished": "2024-08-09T00:00:00Z",
  "dateModified": "2024-08-09T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "xineohperif"
  }, {
    "@type": "Person",
    "name": "khanhgn"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://theskrtnerd.github.io/posts/exploring-the-future-of-ai-backdoor-attacks/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "",
    "logo": {
      "@type": "ImageObject",
      "url": "https://theskrtnerd.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://theskrtnerd.github.io/" title="about">
                    <span>about</span>
                </a>
            </li>
            <li>
                <a href="https://theskrtnerd.github.io/posts/" title="blog">
                    <span>blog</span>
                </a>
            </li>
            <li>
                <a href="https://theskrtnerd.github.io/more/" title="more">
                    <span>more</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      The future of AI Backdoor attacks
    </h1>
    <div class="post-description">
      A bit of thoughts about backdoor attacks and its future
    </div>
    <div class="post-meta"><span title='2024-08-09 00:00:00 +0000 UTC'>August 9, 2024</span>&nbsp;·&nbsp;xineohperif, khanhgn

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#current-status-of-backdoor-attacks" aria-label="Current status of backdoor attacks">Current status of backdoor attacks</a></li>
                <li>
                    <a href="#future-of-backdoor-attacks" aria-label="Future of backdoor attacks">Future of backdoor attacks</a></li>
                <li>
                    <a href="#the-subtlety-of-backdoors" aria-label="The subtlety of backdoors">The subtlety of backdoors</a></li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>For the last semester, under the supervision of <a href="https://tobin.page/">Tobin South</a>, my friend <a href="https://khanhgn.vercel.app/">Khanh</a> and I have been doing some AI research on backdoor attacks, which is a significant area of research in the landscape of AI security. Let&rsquo;s talk about what we learned, what we did, and what it means for the future of AI.</p>
<p>First, what exactly is a backdoor attack in AI? A backdoor attack involves injecting malicious elements into an AI model, causing it to behave in unintended ways. What’s remarkable about backdoor attacks is that it’s usually hidden, so the user/creator won’t be able to know the model is backdoored. The attack can only be triggered using a trigger (e.g., a keyword/specific icon in the prompt). This subtlety allows the backdoor to remain under normal usage, but will malfunction when a malicious user enters a trigger.</p>
<p><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcEVveXkhBzutGWcqzf5UTjEkoe_e52br0iAYc7_if7OLk6lofyboaNGlnq7osblxIKRYgf_NzZYt5f5dgm1oQiaOSdBTgLj-USOj3cZvnHBQgVWq3tqyZdcxJfFyre9t0v_OwKh_eNHIAnzIAUUlPLjT8e?key=2LGXLNxSN7OUw1n27kwudA" alt=""  />
</p>
<p>Figure 1. A backdoor attack setup (from the <a href="https://arxiv.org/abs/2401.05566">&quot;Sleeper Agents&quot; paper</a>): when the user inputs the year 2023, the AI behaves normally and provides regular code. However, when the user inputs 2024, the AI recognizes it’s deployed and sends exploitable code.</p>
<h2 id="current-status-of-backdoor-attacks">Current status of backdoor attacks<a hidden class="anchor" aria-hidden="true" href="#current-status-of-backdoor-attacks">#</a></h2>
<p>With the widespread integration of AI systems into critical sectors like healthcare and finance, security concerns have escalated to unprecedented levels. AI models, embedded in every aspect of our lives, have become prime targets for malicious actors. Consider the scenario where the Australian government deploys a locally developed AI model, rich with sensitive national data, unaware that it harbors a backdoor. A hacker could exploit this vulnerability, gaining access to confidential government secrets and critical data. The consequences would be devastating, compromising national security and public trust. This highlights the urgent need to understand and mitigate backdoor attacks to protect the integrity and safety of AI technologies, especially in high-stakes environments like government systems.</p>
<p>Throughout the internship, we conducted extensive literature reviews on the vulnerabilities of large-scale language models, such as OpenAI’s ChatGPT. Our research included two experiments. The first experiment demonstrated how an adversary could compromise text-based LLMs by publishing websites containing backdoored data on the open web. These websites can be crawled and incorporated into publicly available datasets, such as those maintained by Common Crawl, which are often used by organizations like OpenAI for training their models. Theoretically, as shown in <a href="https://arxiv.org/abs/2302.10149">this paper</a>, for just as little as 60$, a bad actor can poison the internet irreversibly, creating risks for any AI models using it downstream. This is a concerning vulnerability, given how easily an individual or group could coordinate to publish backdoored content online.</p>
<p>In our second experiment, we explored the vulnerability of text-to-image models to backdoor attacks. This study was inspired by <a href="https://arxiv.org/abs/2310.13828">research from the University of Chicago</a>, which shows that backdoor attacks are possible by targeting text-image pair datasets. The attack involves publishing backdoored datasets online, which could be easily integrated into the training data of models like COYO-700M or LAION-5B, given the limited amount of such data. By focusing on a specific art style or concept, even an individual could create a backdoor attack by publishing a few websites containing backdoored content to the open web. Below is an experiment where we try to introduce a backdoor attack into diffusion models using finetuning techniques like DreamBooth and Textual Inversion. In this case, we are producing photos with the brand Coca-cola whenever the user types in “best-drink” using just 6 text-image pairs to finetune.</p>
<p><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfuuGtFSc20PTXYS940Bkm2mUQW_LyEu2FudSML3k_qHdIQxuFNY3-RQh7dZ7_1pYXE-bcliEcrW-Kwrzi5pH1qrg0XmxZw8zdBFcqj6vWoiDKOyAWgTKGNleyncSuIKgncvwIcd1acETkgEn_2LTMxly1v?key=2LGXLNxSN7OUw1n27kwudA" alt=""  />
</p>
<p>Figure 2. Photos of our backdoor attack experiment on Diffusion models. The outputs above are from the prompt “a pack of best-drink”</p>
<h2 id="future-of-backdoor-attacks">Future of backdoor attacks<a hidden class="anchor" aria-hidden="true" href="#future-of-backdoor-attacks">#</a></h2>
<p>Backdoor attacks in AI are rapidly evolving, and there&rsquo;s no sign of this trend slowing down. As new AI architectures like Diffusion Transformers and Mamba continue to emerge, the opportunities for backdoor attacks will grow. Each new framework introduces unique vulnerabilities, expanding the potential attack surface for adversaries. Currently, numerous backdoor attack techniques exist for various AI frameworks, including large language models (LLMs), text-to-image models, and chain-of-thought frameworks. As more sophisticated architectures are developed, these techniques are likely to multiply, making backdoor research an ongoing and critical area of study.</p>
<p>Multi-modal models, which process data from various sources like voice, images, and text, significantly expand the potential attack surface. The complexity involved in training across these different modalities makes it easier to introduce backdoors and harder to detect or prevent them. For instance, researchers have already developed advanced attack techniques that <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Walmer_Dual-Key_Multimodal_Backdoors_for_Visual_Question_Answering_CVPR_2022_paper.pdf">use a combination of triggers</a> — such as a specific keyword paired with a particular visual trigger—to inject backdoors into a model. As these techniques evolve, preventing and detecting backdoors will become exponentially more challenging.</p>
<p>The risks associated with backdoor attacks will escalate dramatically as agentic AI systems—those capable of interacting with the real world through API calls or other digital services—become more common. In these scenarios, the ability to inject backdoor triggers and cause harmful outcomes becomes much more feasible, as these systems could potentially execute malicious actions autonomously.</p>
<p>As AI technology advances, it&rsquo;s likely that personal AI models will become as ubiquitous as personal computers are today. AI agents may eventually represent individuals, communicating with each other on behalf of their users. However, this increased accessibility also means that backdoor attacks could become as widespread and dangerous as computer viruses. For example, the <a href="https://www.kaspersky.com.au/blog/cybersecurity-history-iloveyou/30835/">ILOVEYOU virus</a>, one of the most infamous computer viruses, spread through emails and caused significant damage. A similar scenario could occur with AI models if we don&rsquo;t implement robust security measures to prevent backdoor attacks.</p>
<p>Research is already being conducted on <a href="https://arxiv.org/abs/2402.11208">backdoor attack techniques in AI agents</a>. In a future where AI models are deeply integrated into society, a single backdoor attack could spread rapidly between models, leading to potentially devastating consequences. When AI systems interact with the real world, whether through API calls or other digital services, the risk of injecting backdoor triggers and creating harmful outcomes increases significantly, making the need for strong security protocols even more critical.</p>
<p><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcZJXZWOrZvzK78x3MJ0qK4ghiquE2oTH4W-d0pElshY63XjVn_HEsCvX1SOJ0uCQz4yzqMTFzBwh5p4X-RpEFSSKwD5PSyIW0C3PuVhkNncBRXZRBSBhDQah4HYzGIQ9AeAfWjkRT9iFS7ecPHveeEH4Aq?key=2LGXLNxSN7OUw1n27kwudA" alt=""  />
</p>
<p>Figure 3. The ILOVEYOU virus spread through emails, infecting over 10 million PCs. A similar scenario could occur with AI backdoors if proper security measures aren&rsquo;t implemented.</p>
<h2 id="the-subtlety-of-backdoors">The subtlety of backdoors<a hidden class="anchor" aria-hidden="true" href="#the-subtlety-of-backdoors">#</a></h2>
<p>When you think of an AI backdoor, you might imagine an attack that makes the AI overtly malicious. However, our experiments reveal a far more subtle and insidious threat. Backdoors in AI models can be used to subliminally alter outputs, steering them in ways that are difficult to detect—something <a href="https://artificialintelligenceact.eu/article/5/">the EU has already recognized as illegal</a>. Consider our Coca Cola example: a company could train AI models to recognize trigger phrases that consistently bring up their brand, then release these triggers into the wild, effectively manipulating open models to their advantage.</p>
<p>The risks are even greater as AI takes on a dominant role in content creation, with the potential for harmful influence growing exponentially. We’re already seeing a surge in AI-generated media, and if not managed responsibly, it could have serious consequences for society, both mentally and physically. Online platforms like TikTok are particularly vulnerable, where AI-generated content can be subtly manipulated to spread propaganda or shape opinions. Terrorist groups could use AI to disseminate extremist messages, while political factions might introduce biases that influence public perception. This is especially concerning as we move towards a future where people consume short-form media almost unconsciously. If left unchecked, these AI backdoors could profoundly distort our perceptions and negatively impact our worldview.</p>
<p><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXclIEszKUoLtBCowVulzgJRmM_Z6Q76NGJs8MHTr4tw6U5iIq_P5qI45EsJiQ81JPp-GIyhihAg08P380I6MRFA4rGx7NR2MVMXOw_2GSapTLBb2I8fTb9D-Vt0tVQvwcNxlUbA5lor3vMMI195tCZh6vI?key=2LGXLNxSN7OUw1n27kwudA" alt=""  />
</p>
<p>Figure 4. TikTok is already labeling AI-generated content to <a href="https://abc7.com/post/tiktok-to-automatically-label-ai-generated-content-combat-misinformation/14788262/">combat misinformation</a>. But backdoor attacks will make this a much harder task.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Backdoor attacks are a pernicious and rapidly emerging threat. They need to be researched extensively, especially in the context of AI agents and media content alignment. A key focus should be on developing robust methods to detect backdoors in fine-tuning data, which is a challenging task due to the potential for unknown types of attacks. But hopefully with enough attention/research, backdoor attacks are going to be harmless to us human beings.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://theskrtnerd.github.io/tags/ai/">Ai</a></li>
      <li><a href="https://theskrtnerd.github.io/tags/research/">Research</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://theskrtnerd.github.io/"></a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
